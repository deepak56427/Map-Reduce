[training@localhost ~]$ cd Lahasya
[training@localhost Lahasya]$ ls -l
total 48
-rw-rw-r-- 1 training training 5728 Feb  9 17:22 Distinct.jar
-rw-rw-r-- 1 training training 4523 Feb  9 09:54 DistinctValues.jar
-rw-rw-r-- 1 training training   57 Feb  8 20:24 WC02.txt
-rw-rw-r-- 1 training training 4285 Feb 10 04:54 WC1.jar
-rw-rw-r-- 1 training training 4400 Feb  8 20:11 WC2.jar
-rw-rw-r-- 1 training training   58 Feb  8 20:22 WC2.txt
-rw-rw-r-- 1 training training 4322 Feb 10 05:17 WC.jar
[training@localhost Lahasya]$ hadoop fs -mkdir wc/input
[training@localhost Lahasya]$ hadoop fs -copyFromLocal WC02.txt wc/input
[training@localhost Lahasya]$ hadoop fs -ls wc/input
Found 1 items
-rw-r--r--   1 training supergroup         57 2023-02-10 05:18 wc/input/WC02.txt
[training@localhost Lahasya]$ hadoop jar WC.jar com.wc1.WordCountDriver wc/input wc/output
Usage: WordCount <input dir> <output dir>
[training@localhost Lahasya]$ hadoop jar WC.jar com.wc1.WordCountDriver WordCount/input WordCount/output
Usage: WordCount <input dir> <output dir>
[training@localhost Lahasya]$ hadoop jar WC.jar com.wc1.WordCountDriver wc/input wc/output
Usage: WordCount <input dir> <output dir>
[training@localhost Lahasya]$ vi Sample.txt
[training@localhost Lahasya]$ ls -l
total 52
-rw-rw-r-- 1 training training 5728 Feb  9 17:22 Distinct.jar
-rw-rw-r-- 1 training training 4523 Feb  9 09:54 DistinctValues.jar
-rw-rw-r-- 1 training training   43 Feb 10 05:25 Sample.txt
-rw-rw-r-- 1 training training   57 Feb  8 20:24 WC02.txt
-rw-rw-r-- 1 training training 4285 Feb 10 04:54 WC1.jar
-rw-rw-r-- 1 training training 4400 Feb  8 20:11 WC2.jar
-rw-rw-r-- 1 training training   58 Feb  8 20:22 WC2.txt
-rw-rw-r-- 1 training training 4322 Feb 10 05:17 WC.jar
[training@localhost Lahasya]$ hadoop fs -copyFromLocal Sample.txt wc/input[training@localhost Lahasya]$ hadoop fs -ls wc/inputFound 2 items
-rw-r--r--   1 training supergroup         43 2023-02-10 05:25 wc/input/Sample.txt
-rw-r--r--   1 training supergroup         57 2023-02-10 05:18 wc/input/WC02.txt
[training@localhost Lahasya]$ hadoop jar WC.jar com.wc1.WordCountDriver wc/input wc/output
Usage: WordCount <input dir> <output dir>
[training@localhost Lahasya]$ hadoop jar WC.jar  wc/input wc/output
23/02/10 05:27:56 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
23/02/10 05:27:56 INFO input.FileInputFormat: Total input paths to process : 2
23/02/10 05:27:57 WARN snappy.LoadSnappy: Snappy native library is available
23/02/10 05:27:57 INFO snappy.LoadSnappy: Snappy native library loaded
23/02/10 05:27:57 INFO mapred.JobClient: Running job: job_202302100405_0001
23/02/10 05:27:58 INFO mapred.JobClient:  map 0% reduce 0%
23/02/10 05:28:03 INFO mapred.JobClient:  map 100% reduce 0%
23/02/10 05:28:07 INFO mapred.JobClient:  map 100% reduce 100%
23/02/10 05:28:07 INFO mapred.JobClient: Job complete: job_202302100405_0001
23/02/10 05:28:07 INFO mapred.JobClient: Counters: 32
23/02/10 05:28:07 INFO mapred.JobClient:   File System Counters
23/02/10 05:28:07 INFO mapred.JobClient:     FILE: Number of bytes read=230
23/02/10 05:28:07 INFO mapred.JobClient:     FILE: Number of bytes written=543274
23/02/10 05:28:07 INFO mapred.JobClient:     FILE: Number of read operations=0
23/02/10 05:28:07 INFO mapred.JobClient:     FILE: Number of large read operations=0
23/02/10 05:28:07 INFO mapred.JobClient:     FILE: Number of write operations=0
23/02/10 05:28:07 INFO mapred.JobClient:     HDFS: Number of bytes read=334
23/02/10 05:28:07 INFO mapred.JobClient:     HDFS: Number of bytes written=140
23/02/10 05:28:07 INFO mapred.JobClient:     HDFS: Number of read operations=4
23/02/10 05:28:07 INFO mapred.JobClient:     HDFS: Number of large read operations=0
23/02/10 05:28:07 INFO mapred.JobClient:     HDFS: Number of write operations=1
23/02/10 05:28:07 INFO mapred.JobClient:   Job Counters 
23/02/10 05:28:07 INFO mapred.JobClient:     Launched map tasks=2
23/02/10 05:28:07 INFO mapred.JobClient:     Launched reduce tasks=1
23/02/10 05:28:07 INFO mapred.JobClient:     Data-local map tasks=2
23/02/10 05:28:07 INFO mapred.JobClient:     Total time spent by all maps in occupied slots (ms)=6846
23/02/10 05:28:07 INFO mapred.JobClient:     Total time spent by all reduces in occupied slots (ms)=3927
23/02/10 05:28:07 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
23/02/10 05:28:07 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
23/02/10 05:28:07 INFO mapred.JobClient:   Map-Reduce Framework
23/02/10 05:28:07 INFO mapred.JobClient:     Map input records=5
23/02/10 05:28:07 INFO mapred.JobClient:     Map output records=21
23/02/10 05:28:07 INFO mapred.JobClient:     Map output bytes=182
23/02/10 05:28:07 INFO mapred.JobClient:     Input split bytes=234
23/02/10 05:28:07 INFO mapred.JobClient:     Combine input records=0
23/02/10 05:28:07 INFO mapred.JobClient:     Combine output records=0
23/02/10 05:28:07 INFO mapred.JobClient:     Reduce input groups=11
23/02/10 05:28:07 INFO mapred.JobClient:     Reduce shuffle bytes=236
23/02/10 05:28:07 INFO mapred.JobClient:     Reduce input records=21
23/02/10 05:28:07 INFO mapred.JobClient:     Reduce output records=21
23/02/10 05:28:07 INFO mapred.JobClient:     Spilled Records=42
23/02/10 05:28:07 INFO mapred.JobClient:     CPU time spent (ms)=1360
23/02/10 05:28:07 INFO mapred.JobClient:     Physical memory (bytes) snapshot=446406656
23/02/10 05:28:07 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1195245568
23/02/10 05:28:07 INFO mapred.JobClient:     Total committed heap usage (bytes)=398852096
[training@localhost Lahasya]$ hadoop fs -ls wc/input
Found 2 items
-rw-r--r--   1 training supergroup         43 2023-02-10 05:25 wc/input/Sample.txt
-rw-r--r--   1 training supergroup         57 2023-02-10 05:18 wc/input/WC02.txt
[training@localhost Lahasya]$ hadoop fs -cat ^C
[training@localhost Lahasya]$ hadoop fs -cat wc/input/WC02.txt
This is mapreduce V2
This is also YARN
This is mapreduce
[training@localhost Lahasya]$ hadoop fs -cat wc/input/Sample.txt
This is a word count 
This is a text file 
[training@localhost Lahasya]$ hadoop fs -ls wc/input
Found 2 items
-rw-r--r--   1 training supergroup         43 2023-02-10 05:25 wc/input/Sample.txt
-rw-r--r--   1 training supergroup         57 2023-02-10 05:18 wc/input/WC02.txt
[training@localhost Lahasya]$ hadoop fs -ls wc/output
Found 3 items
-rw-r--r--   1 training supergroup          0 2023-02-10 05:28 wc/output/_SUCCESS
drwxr-xr-x   - training supergroup          0 2023-02-10 05:27 wc/output/_logs
-rw-r--r--   1 training supergroup        140 2023-02-10 05:28 wc/output/part-r-00000
[training@localhost Lahasya]$ hadoop fs -cat wc/output/part-r-00000
This	1
This	1
This	1
This	1
This	1
V2	1
YARN	1
a	1
a	1
also	1
count	1
file	1
is	1
is	1
is	1
is	1
is	1
mapreduce	1
mapreduce	1
text	1
word	1
[training@localhost Lahasya]$ 
